{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqW3PJQxGLZw",
        "outputId": "8a508d4e-73f0-4efe-85ca-3b801027d82d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.0/435.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.0/131.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install labml-nn --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from typing import Optional, List\n",
        "import torch\n",
        "from torch import nn\n",
        "from labml import tracker"
      ],
      "metadata": {
        "id": "4sdkF5q7IRwP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PrepareForMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model: int, heads: int, d_k: int, bias: bool):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(d_model, heads * d_k, bias=bias)\n",
        "        self.heads = heads\n",
        "        self.d_k = d_k\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "#Input has shape [seq_len, batch_size, d_model] or [batch_size, d_model] . We apply the linear transformation to the last dimension and split that into the heads.\n",
        "        head_shape = x.shape[:-1]\n",
        "#Linear transform\n",
        "        x = self.linear(x)\n",
        "\n",
        "#Split last dimension into heads\n",
        "        x = x.view(*head_shape, self.heads, self.d_k)\n",
        "#Output has shape [seq_len, batch_size, heads, d_k] or [batch_size, heads, d_model]\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "AqCysaYYIp7m"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, heads: int, d_model: int, dropout_prob: float = 0.1, bias: bool = True):\n",
        "            super().__init__()\n",
        "            self.d_k = d_model // heads\n",
        "#Number of heads\n",
        "            self.heads = heads\n",
        "#These transform the query , key and value vectors for multi-headed attention.\n",
        "            self.query = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=bias)\n",
        "            self.key = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=bias)\n",
        "            self.value = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=True)\n",
        "#Softmax for attention along the time dimension of key\n",
        "            self.softmax = nn.Softmax(dim=1)\n",
        "#Output layer\n",
        "\n",
        "            self.output = nn.Linear(d_model, d_model)\n",
        "#Dropout\n",
        "            self.dropout = nn.Dropout(dropout_prob)\n",
        "#Scaling factor before the softmax\n",
        "            self.scale = 1 / math.sqrt(self.d_k)\n",
        "#We store attentions so that it can be used for logging, or other computations if needed\n",
        "            self.attn = None\n",
        "#Calculate scores between queries and keys\n",
        "\n",
        "  def get_scores(self, query: torch.Tensor, key: torch.Tensor):\n",
        "    #performing batched matrix multiplication and contraction,\n",
        "         return torch.einsum('ibhd,jbhd->ijbh', query, key)\n"
      ],
      "metadata": {
        "id": "RlMtyJtIT5jk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_mask(self, mask: torch.Tensor, query_shape: List[int], key_shape: List[int]):\n",
        "        assert mask.shape[0] == 1 or mask.shape[0] == query_shape[0]\n",
        "        assert mask.shape[1] == key_shape[0]\n",
        "        assert mask.shape[2] == 1 or mask.shape[2] == query_shape[1]\n",
        "#Same mask applied to all heads.\n",
        "\n",
        "        mask = mask.unsqueeze(-1)\n",
        "#resulting mask has shape [seq_len_q, seq_len_k, batch_size, heads]\n",
        "        return mask\n",
        "\n",
        "def forward(self, *,query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
        "        seq_len, batch_size, _ = query.shape\n",
        "        if mask is not None:\n",
        "            mask = self.prepare_mask(mask, query.shape, key.shape)\n",
        "\n",
        "        query = self.query(query)\n",
        "        key = self.key(key)\n",
        "        value = self.value(value)\n",
        "\n",
        "        scores = self.get_scores(query, key)\n",
        "        scores *= self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "           scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attn = self.softmax(scores)\n",
        "        tracker.debug('attn', attn)\n",
        "        attn = self.dropout(attn)\n",
        "        x = torch.einsum(\"ijbh,jbhd->ibhd\", attn, value)\n",
        "        self.attn = attn.detach()\n",
        "        x = x.reshape(seq_len, batch_size, -1)\n",
        "        return self.output(x)"
      ],
      "metadata": {
        "id": "mduOmm0mkRmI"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}